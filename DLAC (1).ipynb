{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.7.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Moves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import graphical, game\n",
    "\n",
    "# def ai_callback(board, score, moves_left):\n",
    "#     dir = random.randint(0, 1) == 0\n",
    "#     return (random.randint(0, 7 if dir else 6), random.randint(0, 8 if dir else 9), dir)\n",
    "\n",
    "# def transition_callback(board, move, score_delta, next_board, moves_left):\n",
    "#     print(f\"Score: {score_delta} , moves left: {moves_left} \")\n",
    "#     #pass # This can be used to monitor outcomes of moves\n",
    "\n",
    "# def end_of_game_callback(boards, scores, moves, final_score):\n",
    "#     return False # True = play another, False = Done\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     speedup = 1.0\n",
    "#     g = graphical.Game(ai_callback, transition_callback, end_of_game_callback, speedup)\n",
    "#     g.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Action mapping dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [0,1,2,3,4,5,6,7]\n",
    "\n",
    "rows = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "dec = [0,1]\n",
    "\n",
    "actions = []\n",
    "\n",
    "\n",
    "for i in cols:\n",
    "    for j in rows:\n",
    "        for k in dec:\n",
    "            \n",
    "            # dont add furtherest right coloumn with swapping horizontal , as invalid \n",
    "            #if i==7 and k==1:\n",
    "            #    pass\n",
    "            \n",
    "            # if top row, dont add the swapping vertical choice\n",
    "            #if j==0 and k==0;\n",
    "            #    pass\n",
    "            \n",
    "            #else:\n",
    "            actions.append( (i,j,k) )\n",
    "        \n",
    "        \n",
    "print(len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up keys\n",
    "keys = [i for i in range(160)]\n",
    "\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictOfActions = dict(zip(keys, actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictOfActions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictOfActions[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Attempt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess function for board \n",
    "\n",
    "def preprocess_string_int(input_string):\n",
    "    \n",
    "    \"\"\"This will firstly convert the string representation into an interger representation. Then it will convert the interger\n",
    "    representation of the board state into a tensor, to be evaluated in the neural net. \"\"\"\n",
    "    \n",
    "    # removing the \\n as they are really only used to make the text output of board state more readible \n",
    "    new_string = input_string.replace('\\n', \"\")\n",
    "    \n",
    "    #stores ints representation of string\n",
    "    inter = [] \n",
    "    \n",
    "    for i in new_string:\n",
    "        \n",
    "        # a  = 1\n",
    "        if i == 'a':\n",
    "            inter.append(1)\n",
    "        \n",
    "        # b  = 2\n",
    "        if i == 'b':\n",
    "            inter.append(2)\n",
    "            \n",
    "        # c  = 3\n",
    "        if i == 'c':\n",
    "            inter.append(3)   \n",
    "            \n",
    "        # d  = 4\n",
    "        if i == 'd':\n",
    "            inter.append(4) \n",
    "            \n",
    "        # '#' = 5\n",
    "        if i == '#':\n",
    "            inter.append(5) \n",
    "       \n",
    "    \n",
    "    # finally convert to tensor for neural nets\n",
    "    inter_tf = tf.convert_to_tensor(inter)\n",
    "    \n",
    "    inter_tf = tf.expand_dims(inter_tf, 0)\n",
    "   \n",
    "    \n",
    "    return inter_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"abcd\\nbcda#\\nabcda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = preprocess_string_int(test)\n",
    "\n",
    "print(y)\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopefully smart-ish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up of Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up of AC model\n",
    "num_inputs = 80      # board state, theres 80 options \n",
    "num_actions = 160    # allowing all actions\n",
    "num_hidden_1 = 1024  # first hidden layer\n",
    "num_hidden_2 = 512   # second hidden layer \n",
    "\n",
    "\n",
    "inputs = layers.Input(shape=(num_inputs,))\n",
    "\n",
    "#first hidden layer\n",
    "common_1 = layers.Dense(num_hidden_1, activation=\"relu\")(inputs)\n",
    "\n",
    "#second hidden layer\n",
    "common_2 = layers.Dense(num_hidden_2, activation=\"relu\")(common_1)\n",
    "\n",
    "# choosing action \n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common_2)\n",
    "# outputting critic score  \n",
    "critic = layers.Dense(1)(common_2)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Random Choices from AC NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # perfectly fine random \n",
    "\n",
    "\n",
    "import random\n",
    "import graphical, game\n",
    "\n",
    "def ai_callback(board, score, moves_left):\n",
    "    \n",
    "    # pre-process board \n",
    "    integer_board = preprocess_string_int(board)\n",
    "    \n",
    "    print(integer_board)\n",
    "    print(len(integer_board))\n",
    "    \n",
    "    #################################################\n",
    "    \n",
    "    # now actor critic to make decision of choice \n",
    "\n",
    "    action_probs, critic_val = model(integer_board)\n",
    "    \n",
    "    print(action_probs)\n",
    "    \n",
    "    # Sample action from action probability distribution, higher probababilites more chance of getting selected\n",
    "    action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "         \n",
    "    return dictOfActions[action]\n",
    "            \n",
    "    \n",
    "def transition_callback(board, move, score_delta, next_board, moves_left):\n",
    "    print(f\"Score: {score_delta} , moves left: {moves_left} \")\n",
    "    #pass # This can be used to monitor outcomes of moves\n",
    "\n",
    "def end_of_game_callback(boards, scores, moves, final_score):\n",
    "    return False # True = play another, False = Done\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    speedup = 1.0\n",
    "    g = graphical.Game(ai_callback, transition_callback, end_of_game_callback, speedup)\n",
    "    g.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import graphical, game\n",
    "\n",
    "def ai_callback(board, score, moves_left):\n",
    "    \n",
    "    # pre-process board \n",
    "    integer_board = preprocess_string_int(board)\n",
    "    \n",
    "    print(integer_board)\n",
    "    print(len(integer_board))\n",
    "    \n",
    "    #################################################\n",
    "    \n",
    "    # now actor critic to make decision of choice \n",
    "    \n",
    "    # training\n",
    "    gamma = 0.99  # Discount factor for past rewards\n",
    "    max_steps_per_episode = 25\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "    huber_loss = keras.losses.Huber()\n",
    "    action_probs_history = []\n",
    "    critic_value_history = []\n",
    "    rewards_history = []\n",
    "    running_reward = 0\n",
    "    episode_count = 0\n",
    "    \n",
    "    \n",
    "    #while True:  # Run until solved\n",
    "    state = integer_board\n",
    "    episode_reward = 0\n",
    "    \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            # env.render(); Adding this line would show the attempts\n",
    "            # of the agent in a pop up window.\n",
    "\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "\n",
    "            # Predict action probabilities and estimated future rewards\n",
    "            # from environment state\n",
    "            action_probs, critic_value = model(state)\n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "            # Sample action from action probability distribution\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "\n",
    "            # Apply the sampled action in our environment\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Update running reward to check condition for solving\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # Calculate expected value from rewards\n",
    "        # - At each timestep what was the total reward received after that timestep\n",
    "        # - Rewards in the past are discounted by multiplying them with gamma\n",
    "        # - These are the labels for our critic\n",
    "        \n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        # Normalize\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        # Calculating loss values to update our network\n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            \n",
    "            # At this point in history, the critic estimated that we would get a\n",
    "            # total reward = `value` in the future. We took an action with log probability\n",
    "            # of `log_prob` and ended up recieving a total reward = `ret`.\n",
    "            # The actor must be updated so that it predicts an action that leads to\n",
    "            # high rewards (compared to critic's estimate) with high probability.\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "            # The critic must be updated so that it predicts a better estimate of\n",
    "            # the future rewards.\n",
    "            critic_losses.append(\n",
    "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "            )\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Clear the loss and reward history\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "\n",
    "    # Log details\n",
    "    episode_count += 1\n",
    "    if episode_count % 10 == 0:\n",
    "        template = \"running reward: {:.2f} at episode {}\"\n",
    "        print(template.format(running_reward, episode_count))\n",
    "       \n",
    "\n",
    "    dir = random.randint(0, 1) == 0\n",
    "    return (random.randint(0, 7 if dir else 6), random.randint(0, 8 if dir else 9), dir)\n",
    "\n",
    "def transition_callback(board, move, score_delta, next_board, moves_left):\n",
    "    print(f\"Score: {score_delta} , moves left: {moves_left} \")\n",
    "    #pass # This can be used to monitor outcomes of moves\n",
    "\n",
    "def end_of_game_callback(boards, scores, moves, final_score):\n",
    "    return False # True = play another, False = Done\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    speedup = 1.0\n",
    "    g = graphical.Game(ai_callback, transition_callback, end_of_game_callback, speedup)\n",
    "    g.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/examples/rl/actor_critic_cartpole/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
