{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "etivity3_16171659.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jgk2V6o5m7Nn"
      },
      "source": [
        "**Name:** Cathaoir Agnew\n",
        "\n",
        "**ID:** 16171659"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl8Sd8HqnENh"
      },
      "source": [
        "#Task 1 \n",
        "**********************************"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nod68IK9m6fJ",
        "outputId": "c46997c9-8ea1-47a7-eb77-6e6363dabbb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        " \n",
        "!wget https://norvig.com/ngrams/count_1w.txt #unigram corpus\n",
        "!wget https://norvig.com/ngrams/count_2w.txt #bigram corpus\n",
        " \n",
        "filePath1 = \"/content/count_1w.txt\"\n",
        "filePath2 = \"/content/count_2w.txt\"\n",
        " \n",
        "unigrams_df = pd.read_csv(filePath1,sep='\\t',header=None, names=['unigram','count'])\n",
        "bigrams_df = pd.read_csv(filePath2,sep='\\t',header=None, names=['bigram','count'])\n",
        " \n",
        "print(f'Number of unigrams: {unigrams_df.size} Number of Bigrams: {bigrams_df.size}')\n",
        " \n",
        "display(unigrams_df.head(100))\n",
        "display(bigrams_df.head(100))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-12 11:35:31--  https://norvig.com/ngrams/count_1w.txt\n",
            "Resolving norvig.com (norvig.com)... 158.106.138.13\n",
            "Connecting to norvig.com (norvig.com)|158.106.138.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4956241 (4.7M) [text/plain]\n",
            "Saving to: ‘count_1w.txt.3’\n",
            "\n",
            "count_1w.txt.3      100%[===================>]   4.73M  21.1MB/s    in 0.2s    \n",
            "\n",
            "2020-11-12 11:35:31 (21.1 MB/s) - ‘count_1w.txt.3’ saved [4956241/4956241]\n",
            "\n",
            "--2020-11-12 11:35:31--  https://norvig.com/ngrams/count_2w.txt\n",
            "Resolving norvig.com (norvig.com)... 158.106.138.13\n",
            "Connecting to norvig.com (norvig.com)|158.106.138.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5566017 (5.3M) [text/plain]\n",
            "Saving to: ‘count_2w.txt.3’\n",
            "\n",
            "count_2w.txt.3      100%[===================>]   5.31M  22.6MB/s    in 0.2s    \n",
            "\n",
            "2020-11-12 11:35:31 (22.6 MB/s) - ‘count_2w.txt.3’ saved [5566017/5566017]\n",
            "\n",
            "Number of unigrams: 666666 Number of Bigrams: 572716\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>unigram</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the</td>\n",
              "      <td>23135851162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>of</td>\n",
              "      <td>13151942776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>and</td>\n",
              "      <td>12997637966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>to</td>\n",
              "      <td>12136980858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a</td>\n",
              "      <td>9081174698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>like</td>\n",
              "      <td>520585287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>service</td>\n",
              "      <td>519537222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>x</td>\n",
              "      <td>508609523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>than</td>\n",
              "      <td>502609275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>find</td>\n",
              "      <td>502043038</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    unigram        count\n",
              "0       the  23135851162\n",
              "1        of  13151942776\n",
              "2       and  12997637966\n",
              "3        to  12136980858\n",
              "4         a   9081174698\n",
              "..      ...          ...\n",
              "95     like    520585287\n",
              "96  service    519537222\n",
              "97        x    508609523\n",
              "98     than    502609275\n",
              "99     find    502043038\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bigram</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0Uplink verified</td>\n",
              "      <td>523545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0km to</td>\n",
              "      <td>116103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000s of</td>\n",
              "      <td>939476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100s of</td>\n",
              "      <td>539389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100th anniversary</td>\n",
              "      <td>158621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>24th of</td>\n",
              "      <td>327460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>25th anniversary</td>\n",
              "      <td>261023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>25th of</td>\n",
              "      <td>397735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>26th of</td>\n",
              "      <td>271707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>27th of</td>\n",
              "      <td>276619</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               bigram   count\n",
              "0    0Uplink verified  523545\n",
              "1              0km to  116103\n",
              "2            1000s of  939476\n",
              "3             100s of  539389\n",
              "4   100th anniversary  158621\n",
              "..                ...     ...\n",
              "95            24th of  327460\n",
              "96   25th anniversary  261023\n",
              "97            25th of  397735\n",
              "98            26th of  271707\n",
              "99            27th of  276619\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHbsBpPZnNmc"
      },
      "source": [
        "# Task 2 \n",
        "***************************"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhTTIhfrnQWp",
        "outputId": "2e7b2a35-40ca-42c1-83ba-b7697f7a04c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def probability(sentence , verbose = False):\n",
        "\n",
        "  # probability ( wi | wi-1 )  =  count ( wi , wi-1 ) / count (wi-1)\n",
        "\n",
        "  # now need to split up sentence into its individual words, which can be performed by using split() , default split operator is white space \n",
        "  split_sentence = sentence.split()\n",
        "\n",
        "  # print statements\n",
        "  if verbose == True:\n",
        "    print(\"Sentence:\" ,split_sentence)\n",
        "    print(\"\")\n",
        "  sent_probabilty = 1\n",
        "\n",
        "  for i in range(len(split_sentence)-1):\n",
        "\n",
        "    # putting strings into variables to make it easier to look at\n",
        "    uni_string = split_sentence[i]\n",
        "    bi_string = str(split_sentence[i] + \" \" + split_sentence[i+1])\n",
        "\n",
        "    # putting value of counts into variables \n",
        "    uni_gram_count_val = int(unigrams_df.loc[unigrams_df.unigram == uni_string , 'count'].values)\n",
        "    bi_gram_count_val = int(bigrams_df.loc[bigrams_df.bigram == bi_string , 'count'].values)\n",
        "\n",
        "    bigram_prob = bi_gram_count_val / uni_gram_count_val\n",
        "\n",
        "    if verbose == True:\n",
        "      print(f'Unigram count for \"{uni_string}\" = {uni_gram_count_val} ')\n",
        "      print(f'Bigram count for \"{bi_string}\" = {bi_gram_count_val}')\n",
        "      print(f'Bigram Probability for \"{bi_string}\" = {bigram_prob}')\n",
        "\n",
        "      print(\"--------------------------------------------------\")\n",
        "      print(\"\")\n",
        "  \n",
        "    sent_probabilty *= bigram_prob \n",
        "\n",
        "  \n",
        "  if verbose == True:\n",
        "    print(f'Sentence probailty for \"{sentence}\" = {sent_probabilty:.20f}')\n",
        "    print(\"\")\n",
        "    print(\"-----------------------------------------------------\")\n",
        "\n",
        "  return sent_probabilty\n",
        "\n",
        "probability(\"i love you\" , verbose = True) > probability(\"i hate you\" , verbose = True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: ['i', 'love', 'you']\n",
            "\n",
            "Unigram count for \"i\" = 3086225277 \n",
            "Bigram count for \"i love\" = 3979312\n",
            "Bigram Probability for \"i love\" = 0.001289378332053626\n",
            "--------------------------------------------------\n",
            "\n",
            "Unigram count for \"love\" = 201063526 \n",
            "Bigram count for \"love you\" = 5428714\n",
            "Bigram Probability for \"love you\" = 0.02699999402178991\n",
            "--------------------------------------------------\n",
            "\n",
            "Sentence probailty for \"i love you\" = 0.00003481320725727335\n",
            "\n",
            "-----------------------------------------------------\n",
            "Sentence: ['i', 'hate', 'you']\n",
            "\n",
            "Unigram count for \"i\" = 3086225277 \n",
            "Bigram count for \"i hate\" = 876611\n",
            "Bigram Probability for \"i hate\" = 0.0002840398614232463\n",
            "--------------------------------------------------\n",
            "\n",
            "Unigram count for \"hate\" = 21274675 \n",
            "Bigram count for \"hate you\" = 504048\n",
            "Bigram Probability for \"hate you\" = 0.023692394830943365\n",
            "--------------------------------------------------\n",
            "\n",
            "Sentence probailty for \"i hate you\" = 0.00000672958454456599\n",
            "\n",
            "-----------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQFbx_KnASlc"
      },
      "source": [
        "# Task 3 \n",
        "****************************************"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DERTX3_kAR21"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def ShannonVisualization(word, n_sentence = 5):\n",
        "\n",
        "  # n_sentence is the number of words you will allow in a sentence if no exit clause is hit.\n",
        "  # word_counter is the number of current words making up the sentence\n",
        "\n",
        "  word = word \n",
        "  stopper = True\n",
        "  new_dictionary = {}\n",
        "  sentence = word\n",
        "  word_counter = 1\n",
        "\n",
        "  # will run this while stopper == True , that is as long as we get a match of the new word with \n",
        "  # a bi-gram starting with this new word. Example \"men who\", now check does a bi-gram start with \"who\"\n",
        "  # if there is no bi-gram starting with \"who\" , stopper = False, and we stop iterating\n",
        "  # also a word_counter to stop infinte loops, creating infitinly long sentences, this number of n_sentence is user defined.\n",
        "\n",
        "  while stopper == True and word_counter < n_sentence :\n",
        "\n",
        "    print(f'Word = {word}') \n",
        "    new_dictionary = {}\n",
        "    # initalize flag \n",
        "    found_flag = False\n",
        "\n",
        "    # loop through each of the bigrams in the dataframe\n",
        "    for i in bigrams_df.bigram:\n",
        "\n",
        "      # split the string of the bigram, as it is stored as 1 string\n",
        "      split_bigram = i.split()\n",
        "      # since we want to match on the first gram of the bi-gram, we match on split_bigram[0]\n",
        "      match_check = split_bigram[0]\n",
        "      \n",
        "      if word == match_check:\n",
        "        # this is a flag check, checks to see if we have found a match, if we have found a match , found_flag is set to true\n",
        "        if found_flag != True:\n",
        "          found_flag = True\n",
        "\n",
        "        # creates a dictionary with the key as the bi-gram and the value the probabilities for each of the matches\n",
        "\n",
        "        new_dictionary[i] = probability(i)\n",
        "        #new_dictionary[i] = int(bigrams_df.loc[bigrams_df.bigram == i , 'count'].values)\n",
        "\n",
        "        # technically you would not need to calculate the probabilites, since wi-1 is a constant term in the probability calculation. \n",
        "        # And therefore the highest probability would actually just be the highest number of the counts of the bi-grams you are considering.\n",
        "        # However since the algorithm in Dan Jurafsky states Shannon Viz Method uses probabilities, I said I would in fact calculate the probabilites\n",
        "        # even if this is less efficient, as it's technically correct.\n",
        "\n",
        "        \n",
        "        \n",
        "    #print(new_dictionary)\n",
        "\n",
        "    # if we failed to match our new word to the start of a bi-gram, found flag will remain false\n",
        "    # we then make stopper = false, stopping the while loop\n",
        "    if found_flag == False:\n",
        "      stopper = False \n",
        "      print(\"Word had no match\")\n",
        "  \n",
        "    # else statement allows code to run fine if there was a match\n",
        "    else:\n",
        "\n",
        "      # getting the max key & val \n",
        "      max_key = None\n",
        "      max_val = None\n",
        "\n",
        "      for key, val in new_dictionary.items():\n",
        "        if max_val is None or val > max_val:\n",
        "          max_val = val\n",
        "          max_key = key\n",
        "\n",
        "      #print(\"Max Key:\", max_key)\n",
        "      #print(\"Max Val:\", max_val)\n",
        "\n",
        "      print(f'Most frequent bi-gram = {max_key}')\n",
        "\n",
        "      # new word will be the second part of the bigram\n",
        "      word = max_key.split()[1]\n",
        "      # increase counter by 1, as a word will be added\n",
        "      word_counter += 1\n",
        "\n",
        "    # add word to the string, once its a valid one\n",
        "    if found_flag == True:\n",
        "      sentence += \" \" + (word)\n",
        "\n",
        "  # finally print out the sentence\n",
        "  print(\"\\n\",sentence)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbWctwTjqhbq",
        "outputId": "2eab3949-41cd-4a25-8aa0-0e4b8b790705",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ShannonVisualization(\"man\" , 5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word = man\n",
            "Most frequent bi-gram = man who\n",
            "Word = who\n",
            "Most frequent bi-gram = who are\n",
            "Word = are\n",
            "Most frequent bi-gram = are not\n",
            "Word = not\n",
            "Most frequent bi-gram = not be\n",
            "\n",
            " man who are not be\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}