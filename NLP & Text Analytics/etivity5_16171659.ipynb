{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "etivity5_16171659.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J6Pmtkcym8Y"
      },
      "source": [
        "**Name:** Cathaoir Agnew \n",
        "\n",
        "**ID:** 16171659"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUWY9_vtFHfk"
      },
      "source": [
        "#**Task 1**\n",
        "**********************************"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6lIarg8ylzL",
        "outputId": "7f98eb41-6668-4881-ec30-73a2d5f792e8"
      },
      "source": [
        "import pandas as pd\n",
        " \n",
        "!wget http://ptrckprry.com/course/ssd/data/positive-words.txt # positive words\n",
        " \n",
        "filePath1 = \"/content/positive-words.txt\"\n",
        " \n",
        "positive_df = pd.read_csv(filePath1,sep='\\t',header=None, names=['positive'], skiprows = 34 )\n",
        " \n",
        "print(f'Number of words: {positive_df.size}')\n",
        "\n",
        "print(positive_df.head())\n",
        "print(positive_df.tail())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-27 12:52:44--  http://ptrckprry.com/course/ssd/data/positive-words.txt\n",
            "Resolving ptrckprry.com (ptrckprry.com)... 192.30.252.153, 192.30.252.154\n",
            "Connecting to ptrckprry.com (ptrckprry.com)|192.30.252.153|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20630 (20K) [text/plain]\n",
            "Saving to: ‚Äòpositive-words.txt.6‚Äô\n",
            "\n",
            "\rpositive-words.txt.   0%[                    ]       0  --.-KB/s               \rpositive-words.txt. 100%[===================>]  20.15K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-11-27 12:52:44 (1.55 MB/s) - ‚Äòpositive-words.txt.6‚Äô saved [20630/20630]\n",
            "\n",
            "Number of words: 2006\n",
            "    positive\n",
            "0         a+\n",
            "1     abound\n",
            "2    abounds\n",
            "3  abundance\n",
            "4   abundant\n",
            "      positive\n",
            "2001  youthful\n",
            "2002      zeal\n",
            "2003    zenith\n",
            "2004      zest\n",
            "2005     zippy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwHPJ4lw4kC6"
      },
      "source": [
        "Stackover flow for solving an error I was having. \n",
        "\n",
        "https://stackoverflow.com/questions/19699367/for-line-in-results-in-unicodedecodeerror-utf-8-codec-cant-decode-byte"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49e1KYg411XA",
        "outputId": "b1d492d9-373f-4a8b-eef9-c9171b72078f"
      },
      "source": [
        "!wget http://ptrckprry.com/course/ssd/data/negative-words.txt # negative words\n",
        "\n",
        "filePath2 = \"/content/negative-words.txt\"\n",
        "\n",
        "negative_df = pd.read_csv(filePath2,sep='\\t',header=None, names=['negative'], skiprows = 34 , encoding=\"ISO-8859-1\")\n",
        "print(f'Number of words: {negative_df.size}')\n",
        "\n",
        "print(negative_df.head())\n",
        "print(negative_df.tail())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-27 12:52:44--  http://ptrckprry.com/course/ssd/data/negative-words.txt\n",
            "Resolving ptrckprry.com (ptrckprry.com)... 192.30.252.153, 192.30.252.154\n",
            "Connecting to ptrckprry.com (ptrckprry.com)|192.30.252.153|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46299 (45K) [text/plain]\n",
            "Saving to: ‚Äònegative-words.txt.6‚Äô\n",
            "\n",
            "\rnegative-words.txt.   0%[                    ]       0  --.-KB/s               \rnegative-words.txt. 100%[===================>]  45.21K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-11-27 12:52:44 (1.76 MB/s) - ‚Äònegative-words.txt.6‚Äô saved [46299/46299]\n",
            "\n",
            "Number of words: 4783\n",
            "     negative\n",
            "0     2-faced\n",
            "1     2-faces\n",
            "2    abnormal\n",
            "3     abolish\n",
            "4  abominable\n",
            "       negative\n",
            "4778       zaps\n",
            "4779     zealot\n",
            "4780    zealous\n",
            "4781  zealously\n",
            "4782     zombie\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeziqT-Q5GQK",
        "outputId": "3f5b3d34-fe88-4a60-9942-030468e907ed"
      },
      "source": [
        "print(f'Number of positive words: {positive_df.size} , Number of negative words: {negative_df.size}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of positive words: 2006 , Number of negative words: 4783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAn0h4W2FKss"
      },
      "source": [
        "##Rule based sentiment analyzer function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fakb0pEVFQOw"
      },
      "source": [
        "def sentimentAnalyzer(string, word_list = False):\n",
        "\n",
        "  # added a word_list so you can output what words are positive or negative if you want. More of a feature than debugging. \n",
        "\n",
        "  print(f'string= {string}')\n",
        "\n",
        "  # first split into individual words \n",
        "  split = string.split() \n",
        "\n",
        "  # initialize counters\n",
        "  positive_score = 0\n",
        "  negative_score = 0\n",
        "\n",
        "  positive_words = []\n",
        "  negative_words = []\n",
        "\n",
        "  # loop through each word of original string \n",
        "  for i in split:\n",
        "\n",
        "    # check if word is in positive dataframe, if it is add to counter\n",
        "    for j in positive_df.positive:\n",
        "\n",
        "      # check if word matches with current word in positive column, if theres a match, update counter & list, then break out as no point checking rest of loop, as already matched on word\n",
        "      if i == j:\n",
        "        positive_score +=1\n",
        "        positive_words.append(i)\n",
        "        break\n",
        "\n",
        "\n",
        "    # check if word is in negative dataframe, if it is add to counter\n",
        "    for k in negative_df.negative:\n",
        "\n",
        "    # check if word matches with current word in negative column, if theres a match, update counter & list, then break out as no point checking rest of loop, as already matched on word\n",
        "      if i == k:\n",
        "        negative_score += 1 \n",
        "        negative_words.append(i)\n",
        "        break \n",
        "\n",
        "    #if didnt match on either just loops back through, no need to handle this because it uses if statements\n",
        "\n",
        "  if positive_score > negative_score:\n",
        "    print('positive sentiment üòä')\n",
        "    \n",
        "    #calculating confidence, number of postive words divided by total words\n",
        "    confidence = (positive_score/ len(split))\n",
        "    print(f'confidence = {confidence}')\n",
        "    \n",
        "    if word_list == True:\n",
        "      print(\"\\nList of postive words =\", positive_words)\n",
        "\n",
        "\n",
        "  if negative_score > positive_score:\n",
        "    print('negative sentiment üò¢')\n",
        "    \n",
        "    #calculating confidence, number of postive words divided by total words\n",
        "    confidence = (negative_score / len(split))\n",
        "    print(f'confidence = {confidence}')\n",
        "\n",
        "    if word_list == True:\n",
        "      print(\"\\nList of negative words \",negative_words)\n",
        "\n",
        "\n",
        "  if negative_score == positive_score:\n",
        "      print(\"neutral sentiment üòê\")\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iGQZVGvHs38",
        "outputId": "6c8cf349-ad11-4bf3-d61e-09ef5f4e5cd5"
      },
      "source": [
        "sentimentAnalyzer(\"NLP is cool\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "string= NLP is cool\n",
            "positive sentiment üòä\n",
            "confidence = 0.3333333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_b50lR1PQCk",
        "outputId": "c54411ef-698b-4632-946d-3ef3a95b21ac"
      },
      "source": [
        "sentimentAnalyzer(\"NLP is cool and useful\" ,word_list = True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "string= NLP is cool and useful\n",
            "positive sentiment üòä\n",
            "confidence = 0.4\n",
            "\n",
            "List of postive words = ['cool', 'useful']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6roJx3YPPTVZ",
        "outputId": "7247deba-25bb-418f-aa8c-34e0a17ae84c"
      },
      "source": [
        "sentimentAnalyzer(\"NLP is hard\", word_list = True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "string= NLP is hard\n",
            "negative sentiment üò¢\n",
            "confidence = 0.3333333333333333\n",
            "\n",
            "List of negative words  ['hard']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTKxaObsPYzF",
        "outputId": "51c53c63-1b68-44ff-e0a7-39c4046a9638"
      },
      "source": [
        "sentimentAnalyzer(\"NLP is hard and useless\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "string= NLP is hard and useless\n",
            "negative sentiment üò¢\n",
            "confidence = 0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y6i_N5VTUW_"
      },
      "source": [
        "#**Task 2**\n",
        "**********************\n",
        "\n",
        "Write a NaiveBayes function which takes the sample training and testing documents shown in the table below as input, and uses the Naive Bayes algorithm to classify the test documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9FULWZZRUfY"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def naiveBayesClassifier(trainingSet, testSet):\n",
        "\n",
        "  #################################################################################################\n",
        "\n",
        "  # This section is preprocessing, taking in strings, and forming mega doc for Ireland and GB\n",
        "\n",
        "  # split into megaDoc firstly \n",
        "  gb_string = \"\"\n",
        "  ie_string = \"\"\n",
        "\n",
        "  # intitalzie a counter for calculating prior probability\n",
        "  counter_gb = 0\n",
        "  counter_ie = 0\n",
        "\n",
        "  # iterate through training data \n",
        "  for i in trainingSet:\n",
        "\n",
        "    # if its in reference to GB, add to relevant string & add counter \n",
        "    if str(i[1]) == 'GB':\n",
        "      gb_string += \" \" + str(i[0].lower())\n",
        "      counter_gb += 1\n",
        "\n",
        "    # if its in reference to IE, add to relevant string & add counter  \n",
        "    if str(i[1]) == 'IE':\n",
        "      ie_string += \" \" + str(i[0].lower())\n",
        "      counter_ie += 1\n",
        "\n",
        "  print(\"megaDocGB=\", gb_string)\n",
        "  print(\"megaDocIE=\", ie_string)\n",
        "\n",
        "  #################################################################################################\n",
        "\n",
        "  # This section is calculating |V| , the unique words in the training data\n",
        "\n",
        "  all_words = gb_string + ie_string\n",
        "\n",
        "  # firsly want to calculate |v| of all words\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts([all_words])\n",
        "\n",
        "  print(\"\\nV = \", dict(tokenizer.word_counts))\n",
        "\n",
        "  v = len(tokenizer.word_counts)\n",
        "  print(\"\\n|V| = \",v)\n",
        "\n",
        "  # learned in the lecture you could have simply split the words and used counts and stored these values in a dictionary, but nltk also just does this \n",
        "\n",
        " #################################################################################################\n",
        "\n",
        "  # This section is calculating prior probabilites\n",
        "\n",
        "  # then calculate priors\n",
        "  probGB = ( counter_gb / (counter_gb + counter_ie) )\n",
        "  probIE = ( counter_ie / (counter_gb + counter_ie) )\n",
        "  \n",
        "  print(f'\\nprobGB = {probGB}  probIE = {probIE}')\n",
        "\n",
        "\n",
        "  #################################################################################################\n",
        "\n",
        "  # This section is tokenizing both Ireland & GB strings\n",
        "  # as tokens will have word counts beside the words its a nice way to store it\n",
        "\n",
        "  # learned in the lecture you could have simply split the words and used counts and stored these values in a dictionary, but nltk also just does this \n",
        "\n",
        "  tokenizer_gb = Tokenizer()\n",
        "  tokenizer_gb.fit_on_texts([gb_string])\n",
        "  dict_of_gb_token = dict(tokenizer_gb.word_counts)\n",
        "  print(\"\\nGB_BOW =\" , dict_of_gb_token )\n",
        "\n",
        "  tokenizer_ie = Tokenizer()\n",
        "  tokenizer_ie.fit_on_texts([ie_string])\n",
        "  dict_of_ie_token = dict(tokenizer_ie.word_counts)\n",
        "  print(\"IE_BOW =\" , dict_of_ie_token )\n",
        "\n",
        "\n",
        "\n",
        "  #################################################################################################\n",
        "\n",
        "  # This section is getting the number of words for each Ireland & GB training strings\n",
        "\n",
        "  # calculate the number of total words in GB training set \n",
        "  total_val_of_GB_training_words  = sum(dict_of_gb_token[item] for item in dict_of_gb_token)\n",
        "\n",
        "  # calculate the number of total words in IE training set \n",
        "  total_val_of_IE_training_words  = sum(dict_of_ie_token[item] for item in dict_of_ie_token)\n",
        "\n",
        "\n",
        "  #################################################################################################\n",
        "\n",
        "  # This section is calculating conditional probabilites\n",
        "\n",
        "\n",
        "  for k in testSet:\n",
        "\n",
        "    # initalize these before going into each of test set \n",
        "    running_prob_gb = 1\n",
        "    running_prob_ie = 1\n",
        " \n",
        "    print(\"\\n----------------------------------------------------\")\n",
        "\n",
        "    print(f'Test document = {k}')\n",
        "\n",
        "    # print a space for seperation\n",
        "    print(\" \")\n",
        "\n",
        "    # this will give a list of all words in the sentence, need to lower the words since nltk lowers tokens\n",
        "    test_words = k[0].lower().split()\n",
        "\n",
        "\n",
        "    # need to iterate for each word & calculate probablies of each GB & IE\n",
        "    for j in test_words:\n",
        "\n",
        "\n",
        "      #need to calculate both IE set & GB set \n",
        "\n",
        "      # GB Set first, going to put a try catch on, if is in dictionary will access dictionary using key and gets word count fine from the value,\n",
        "      # if its not exception will occur, manually set to 0, as word doesnt exist in GB set in training data\n",
        "      \n",
        "      try:\n",
        "\n",
        "        probability_word_gb  = ( ( dict_of_gb_token[str(j)] + 1 ) / (total_val_of_GB_training_words + v) )\n",
        "\n",
        "      except:\n",
        "\n",
        "        # if the word is not in the dictionary of tokens its count is set to 0\n",
        "        probability_word_gb  = ( ( 0 + 1 ) / (total_val_of_GB_training_words + v ) )\n",
        "\n",
        "\n",
        "      # IE Set now, going to put a try catch on, if is in dictionary will acces dictionary using key and gets word count fine from the value\n",
        "      # if its not exception will occur, manually set to 0, as word doesnt exist in IE set in training data\n",
        "      try:\n",
        "\n",
        "        probability_word_ie  = ( ( dict_of_ie_token[str(j)] + 1 ) / (total_val_of_IE_training_words + v) )\n",
        "\n",
        "      except:\n",
        "        \n",
        "        # if the word is not in the dictionary of tokens its count is set to 0\n",
        "        probability_word_ie  = ( ( 0 + 1 ) / (total_val_of_IE_training_words + v ) )\n",
        "\n",
        "\n",
        "      # better looking string formatting \n",
        "      print(f\" word '{j :<20}'     , wordConditionalProbGB= '{probability_word_gb :<20}' , wordConditionalProbIE= '{probability_word_ie}' \")\n",
        "\n",
        "\n",
        "      # calculates running probabilies of each of the words \n",
        "      running_prob_gb *= probability_word_gb\n",
        "      running_prob_ie *= probability_word_ie \n",
        "\n",
        "\n",
        "    #finally calculate which class it belongs too =  prior * conditional for each word \n",
        "\n",
        "    final_probability_GB = probGB * running_prob_gb\n",
        "    final_probability_IE = probIE * running_prob_ie\n",
        "\n",
        "\n",
        "\n",
        "    print(f'\\ndocProbGB = {final_probability_GB: .7f} , docProbIE = {final_probability_IE : .7f}')\n",
        "\n",
        "    # rounding to account for any floating point arithmetic errors of how they are stored \n",
        "\n",
        "    if np.round(final_probability_IE ,15) == np.round(final_probability_GB,15) :\n",
        "      print(f'Infered class = IE,GB')\n",
        "\n",
        "    elif final_probability_GB > final_probability_IE:\n",
        "      print(f'Infered class = GB')\n",
        "\n",
        "    elif final_probability_IE > final_probability_GB:\n",
        "      print(f'Infered class = IE')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXyFxNSQZ1nr"
      },
      "source": [
        "***More or less how probabilites will work with our given test data:***\n",
        "*****************************************\n",
        "if word doesnt exist in training data we are not required to handle this, as Arash said on forums. \n",
        "\n",
        "if word doesnt exist in specific Set IE or GB = (0+1) / ( 12+ 13 ) = 1/25 ~ 0.04\n",
        "\n",
        "if 1 word exist in training data for the set either IE or GB = (1+1) / ( 12+ 13 ) = 2/25 ~ 0.08\n",
        "\n",
        "if 2 word exist in training data for the set either IE or GB = (2+1) / ( 12+ 13 ) = 3/25 ~ 0.12"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TH3dY3DSEK2",
        "outputId": "3e53d71a-f212-4048-cdf0-9c8a3e3c318c"
      },
      "source": [
        "trainingSet = [ ('London is the capital of GB' , 'GB'), ('Oxford is a city in GB' , 'GB') , ('Dublin is the capital of Ireland' , 'IE'), ('Limerick is a city in Ireland', 'IE') ]\n",
        "                                                                                                                                         \n",
        "testSet = [('University of Limerick' , '?') , ('University College Dublin' , '?') , ('Imperial College London' , '?') , ('University of Oxford', '?') , ('Ireland & GB', '?')] \n",
        "\n",
        "\n",
        "naiveBayesClassifier(trainingSet, testSet)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "megaDocGB=  london is the capital of gb oxford is a city in gb\n",
            "megaDocIE=  dublin is the capital of ireland limerick is a city in ireland\n",
            "\n",
            "V =  {'london': 1, 'is': 4, 'the': 2, 'capital': 2, 'of': 2, 'gb': 2, 'oxford': 1, 'a': 2, 'city': 2, 'in': 2, 'dublin': 1, 'ireland': 2, 'limerick': 1}\n",
            "\n",
            "|V| =  13\n",
            "\n",
            "probGB = 0.5  probIE = 0.5\n",
            "\n",
            "GB_BOW = {'london': 1, 'is': 2, 'the': 1, 'capital': 1, 'of': 1, 'gb': 2, 'oxford': 1, 'a': 1, 'city': 1, 'in': 1}\n",
            "IE_BOW = {'dublin': 1, 'is': 2, 'the': 1, 'capital': 1, 'of': 1, 'ireland': 2, 'limerick': 1, 'a': 1, 'city': 1, 'in': 1}\n",
            "\n",
            "----------------------------------------------------\n",
            "Test document = ('University of Limerick', '?')\n",
            " \n",
            " word 'university          '     , wordConditionalProbGB= '0.04                ' , wordConditionalProbIE= '0.04' \n",
            " word 'of                  '     , wordConditionalProbGB= '0.08                ' , wordConditionalProbIE= '0.08' \n",
            " word 'limerick            '     , wordConditionalProbGB= '0.04                ' , wordConditionalProbIE= '0.08' \n",
            "\n",
            "docProbGB =  0.0000640 , docProbIE =  0.0001280\n",
            "Infered class = IE\n",
            "\n",
            "----------------------------------------------------\n",
            "Test document = ('University College Dublin', '?')\n",
            " \n",
            " word 'university          '     , wordConditionalProbGB= '0.04                ' , wordConditionalProbIE= '0.04' \n",
            " word 'college             '     , wordConditionalProbGB= '0.04                ' , wordConditionalProbIE= '0.04' \n",
            " word 'dublin              '     , wordConditionalProbGB= '0.04                ' , wordConditionalProbIE= '0.08' \n",
            "\n",
            "docProbGB =  0.0000320 , docProbIE =  0.0000640\n",
            "Infered class = IE\n",
            "\n",
            "----------------------------------------------------\n",
            "Test document = ('Imperial College London', '?')\n",
            " \n",
            " word 'imperial            '     , wordConditionalProbGB= '0.04                ' , wordConditionalProbIE= '0.04' \n",
            " word 'college             '     , wordConditionalProbGB= '0.04                ' , wordConditionalProbIE= '0.04' \n",
            " word 'london              '     , wordConditionalProbGB= '0.08                ' , wordConditionalProbIE= '0.04' \n",
            "\n",
            "docProbGB =  0.0000640 , docProbIE =  0.0000320\n",
            "Infered class = GB\n",
            "\n",
            "----------------------------------------------------\n",
            "Test document = ('University of Oxford', '?')\n",
            " \n",
            " word 'university          '     , wordConditionalProbGB= '0.04                ' , wordConditionalProbIE= '0.04' \n",
            " word 'of                  '     , wordConditionalProbGB= '0.08                ' , wordConditionalProbIE= '0.08' \n",
            " word 'oxford              '     , wordConditionalProbGB= '0.08                ' , wordConditionalProbIE= '0.04' \n",
            "\n",
            "docProbGB =  0.0001280 , docProbIE =  0.0000640\n",
            "Infered class = GB\n",
            "\n",
            "----------------------------------------------------\n",
            "Test document = ('Ireland & GB', '?')\n",
            " \n",
            " word 'ireland             '     , wordConditionalProbGB= '0.04                ' , wordConditionalProbIE= '0.12' \n",
            " word '&                   '     , wordConditionalProbGB= '0.04                ' , wordConditionalProbIE= '0.04' \n",
            " word 'gb                  '     , wordConditionalProbGB= '0.12                ' , wordConditionalProbIE= '0.04' \n",
            "\n",
            "docProbGB =  0.0000960 , docProbIE =  0.0000960\n",
            "Infered class = IE,GB\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}