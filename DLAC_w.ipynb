{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Resource "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/examples/rl/actor_critic_cartpole/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondary Resources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything You Need To Master Actor Critic Methods\n",
    "https://www.youtube.com/watch?v=LawaN3BdI00\n",
    "\n",
    "https://github.com/philtabor/Youtube-Code-Repository/blob/master/ReinforcementLearning/PolicyGradient/actor_critic/tensorflow2/networks.py\n",
    "\n",
    "https://github.com/sweetice/Deep-reinforcement-learning-with-pytorch\n",
    "\n",
    "https://github.com/hermesdt/reinforcement-learning/blob/master/a2c/cartpole_a2c_episodic.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next steps are\n",
    "# 1. Get training working per example in end game call back\n",
    "# 2. Add CNN to start of network       - hopefully done \n",
    "# 3. Train with animations turned off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.7.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input, Conv2D, Conv1D, Flatten\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Moves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import graphical, game\n",
    "\n",
    "# def ai_callback(board, score, moves_left):\n",
    "#     dir = random.randint(0, 1) == 0\n",
    "#     return (random.randint(0, 7 if dir else 6), random.randint(0, 8 if dir else 9), dir)\n",
    "\n",
    "# def transition_callback(board, move, score_delta, next_board, moves_left):\n",
    "#     print(f\"Score: {score_delta} , moves left: {moves_left} \")\n",
    "#     #pass # This can be used to monitor outcomes of moves\n",
    "\n",
    "# def end_of_game_callback(boards, scores, moves, final_score):\n",
    "#     return False # True = play another, False = Done\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     speedup = 1.0\n",
    "#     g = graphical.Game(ai_callback, transition_callback, end_of_game_callback, speedup)\n",
    "#     g.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Action mapping dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n"
     ]
    }
   ],
   "source": [
    "cols = [0,1,2,3,4,5,6,7]\n",
    "\n",
    "rows = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "\n",
    "# 0 = Horizontal swap  , 1 = vertical swap\n",
    "dec = [0,1]\n",
    "\n",
    "actions = []\n",
    "\n",
    "\n",
    "for i in cols:\n",
    "    for j in rows:\n",
    "        for k in dec:\n",
    "            \n",
    "            # dont add furtherest right coloumn with swapping horizontal , as invalid \n",
    "            if i==7 and k==0:\n",
    "                continue\n",
    "            \n",
    "            # if top row, dont add the swapping vertical choice, as invalid\n",
    "            if j==0 and k==1:\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                actions.append( (i,j,k) )\n",
    "        \n",
    "        \n",
    "print(len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, 0), (0, 1, 0), (0, 1, 1), (0, 2, 0), (0, 2, 1), (0, 3, 0), (0, 3, 1), (0, 4, 0), (0, 4, 1), (0, 5, 0), (0, 5, 1), (0, 6, 0), (0, 6, 1), (0, 7, 0), (0, 7, 1), (0, 8, 0), (0, 8, 1), (0, 9, 0), (0, 9, 1), (1, 0, 0), (1, 1, 0), (1, 1, 1), (1, 2, 0), (1, 2, 1), (1, 3, 0), (1, 3, 1), (1, 4, 0), (1, 4, 1), (1, 5, 0), (1, 5, 1), (1, 6, 0), (1, 6, 1), (1, 7, 0), (1, 7, 1), (1, 8, 0), (1, 8, 1), (1, 9, 0), (1, 9, 1), (2, 0, 0), (2, 1, 0), (2, 1, 1), (2, 2, 0), (2, 2, 1), (2, 3, 0), (2, 3, 1), (2, 4, 0), (2, 4, 1), (2, 5, 0), (2, 5, 1), (2, 6, 0), (2, 6, 1), (2, 7, 0), (2, 7, 1), (2, 8, 0), (2, 8, 1), (2, 9, 0), (2, 9, 1), (3, 0, 0), (3, 1, 0), (3, 1, 1), (3, 2, 0), (3, 2, 1), (3, 3, 0), (3, 3, 1), (3, 4, 0), (3, 4, 1), (3, 5, 0), (3, 5, 1), (3, 6, 0), (3, 6, 1), (3, 7, 0), (3, 7, 1), (3, 8, 0), (3, 8, 1), (3, 9, 0), (3, 9, 1), (4, 0, 0), (4, 1, 0), (4, 1, 1), (4, 2, 0), (4, 2, 1), (4, 3, 0), (4, 3, 1), (4, 4, 0), (4, 4, 1), (4, 5, 0), (4, 5, 1), (4, 6, 0), (4, 6, 1), (4, 7, 0), (4, 7, 1), (4, 8, 0), (4, 8, 1), (4, 9, 0), (4, 9, 1), (5, 0, 0), (5, 1, 0), (5, 1, 1), (5, 2, 0), (5, 2, 1), (5, 3, 0), (5, 3, 1), (5, 4, 0), (5, 4, 1), (5, 5, 0), (5, 5, 1), (5, 6, 0), (5, 6, 1), (5, 7, 0), (5, 7, 1), (5, 8, 0), (5, 8, 1), (5, 9, 0), (5, 9, 1), (6, 0, 0), (6, 1, 0), (6, 1, 1), (6, 2, 0), (6, 2, 1), (6, 3, 0), (6, 3, 1), (6, 4, 0), (6, 4, 1), (6, 5, 0), (6, 5, 1), (6, 6, 0), (6, 6, 1), (6, 7, 0), (6, 7, 1), (6, 8, 0), (6, 8, 1), (6, 9, 0), (6, 9, 1), (7, 1, 1), (7, 2, 1), (7, 3, 1), (7, 4, 1), (7, 5, 1), (7, 6, 1), (7, 7, 1), (7, 8, 1), (7, 9, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141]\n"
     ]
    }
   ],
   "source": [
    "# set up keys\n",
    "keys = [i for i in range(142)]\n",
    "\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictOfActions = dict(zip(keys, actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: (0, 0, 0), 1: (0, 1, 0), 2: (0, 1, 1), 3: (0, 2, 0), 4: (0, 2, 1), 5: (0, 3, 0), 6: (0, 3, 1), 7: (0, 4, 0), 8: (0, 4, 1), 9: (0, 5, 0), 10: (0, 5, 1), 11: (0, 6, 0), 12: (0, 6, 1), 13: (0, 7, 0), 14: (0, 7, 1), 15: (0, 8, 0), 16: (0, 8, 1), 17: (0, 9, 0), 18: (0, 9, 1), 19: (1, 0, 0), 20: (1, 1, 0), 21: (1, 1, 1), 22: (1, 2, 0), 23: (1, 2, 1), 24: (1, 3, 0), 25: (1, 3, 1), 26: (1, 4, 0), 27: (1, 4, 1), 28: (1, 5, 0), 29: (1, 5, 1), 30: (1, 6, 0), 31: (1, 6, 1), 32: (1, 7, 0), 33: (1, 7, 1), 34: (1, 8, 0), 35: (1, 8, 1), 36: (1, 9, 0), 37: (1, 9, 1), 38: (2, 0, 0), 39: (2, 1, 0), 40: (2, 1, 1), 41: (2, 2, 0), 42: (2, 2, 1), 43: (2, 3, 0), 44: (2, 3, 1), 45: (2, 4, 0), 46: (2, 4, 1), 47: (2, 5, 0), 48: (2, 5, 1), 49: (2, 6, 0), 50: (2, 6, 1), 51: (2, 7, 0), 52: (2, 7, 1), 53: (2, 8, 0), 54: (2, 8, 1), 55: (2, 9, 0), 56: (2, 9, 1), 57: (3, 0, 0), 58: (3, 1, 0), 59: (3, 1, 1), 60: (3, 2, 0), 61: (3, 2, 1), 62: (3, 3, 0), 63: (3, 3, 1), 64: (3, 4, 0), 65: (3, 4, 1), 66: (3, 5, 0), 67: (3, 5, 1), 68: (3, 6, 0), 69: (3, 6, 1), 70: (3, 7, 0), 71: (3, 7, 1), 72: (3, 8, 0), 73: (3, 8, 1), 74: (3, 9, 0), 75: (3, 9, 1), 76: (4, 0, 0), 77: (4, 1, 0), 78: (4, 1, 1), 79: (4, 2, 0), 80: (4, 2, 1), 81: (4, 3, 0), 82: (4, 3, 1), 83: (4, 4, 0), 84: (4, 4, 1), 85: (4, 5, 0), 86: (4, 5, 1), 87: (4, 6, 0), 88: (4, 6, 1), 89: (4, 7, 0), 90: (4, 7, 1), 91: (4, 8, 0), 92: (4, 8, 1), 93: (4, 9, 0), 94: (4, 9, 1), 95: (5, 0, 0), 96: (5, 1, 0), 97: (5, 1, 1), 98: (5, 2, 0), 99: (5, 2, 1), 100: (5, 3, 0), 101: (5, 3, 1), 102: (5, 4, 0), 103: (5, 4, 1), 104: (5, 5, 0), 105: (5, 5, 1), 106: (5, 6, 0), 107: (5, 6, 1), 108: (5, 7, 0), 109: (5, 7, 1), 110: (5, 8, 0), 111: (5, 8, 1), 112: (5, 9, 0), 113: (5, 9, 1), 114: (6, 0, 0), 115: (6, 1, 0), 116: (6, 1, 1), 117: (6, 2, 0), 118: (6, 2, 1), 119: (6, 3, 0), 120: (6, 3, 1), 121: (6, 4, 0), 122: (6, 4, 1), 123: (6, 5, 0), 124: (6, 5, 1), 125: (6, 6, 0), 126: (6, 6, 1), 127: (6, 7, 0), 128: (6, 7, 1), 129: (6, 8, 0), 130: (6, 8, 1), 131: (6, 9, 0), 132: (6, 9, 1), 133: (7, 1, 1), 134: (7, 2, 1), 135: (7, 3, 1), 136: (7, 4, 1), 137: (7, 5, 1), 138: (7, 6, 1), 139: (7, 7, 1), 140: (7, 8, 1), 141: (7, 9, 1)}\n"
     ]
    }
   ],
   "source": [
    "print(dictOfActions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 3, 0)\n"
     ]
    }
   ],
   "source": [
    "print(dictOfActions[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Attempt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess function for board \n",
    "\n",
    "def preprocess_string_int(input_string):\n",
    "    \n",
    "    \"\"\"This will firstly convert the string representation into an interger representation. Then it will convert the interger\n",
    "    representation of the board state into a tensor, to be evaluated in the neural net. \"\"\"\n",
    "    \n",
    "    # removing the \\n as they are really only used to make the text output of board state more readible \n",
    "    new_string = input_string.replace('\\n', \"\")\n",
    "    \n",
    "    #stores ints representation of string\n",
    "    inter = [] \n",
    "    \n",
    "    for i in new_string:\n",
    "        \n",
    "        # a  = 1\n",
    "        if i == 'a':\n",
    "            inter.append(1)\n",
    "        \n",
    "        # b  = 2\n",
    "        if i == 'b':\n",
    "            inter.append(2)\n",
    "            \n",
    "        # c  = 3\n",
    "        if i == 'c':\n",
    "            inter.append(3)   \n",
    "            \n",
    "        # d  = 4\n",
    "        if i == 'd':\n",
    "            inter.append(4) \n",
    "            \n",
    "        # '#' = 5\n",
    "        if i == '#':\n",
    "            inter.append(5) \n",
    "       \n",
    "    \n",
    "    # finally convert to tensor for neural nets\n",
    "    inter_tf = tf.convert_to_tensor(inter)\n",
    "    \n",
    "    # make it ready for input into NN\n",
    "    inter_tf = tf.expand_dims(inter_tf, 0)\n",
    "    \n",
    "    #one-hot encode board \n",
    "    inter_tf_ohe = to_categorical(inter_tf)\n",
    "   \n",
    "    \n",
    "    return inter_tf_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"abcd\\nbcda#\\nabcda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0. 0. 0.]]]\n",
      "(1, 14, 6)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "y = preprocess_string_int(test)\n",
    "\n",
    "print(y)\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up of CNN Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 142    # taking out some invalid moves\n",
    "\n",
    "inputs = keras.Input(shape=(80,6,))\n",
    "\n",
    "x=inputs\n",
    "\n",
    "#cnn layers\n",
    "x = keras.layers.Conv1D(32, kernel_size=3, name='Conv_1')(x)\n",
    "x = keras.layers.LeakyReLU(0.1)(x)      \n",
    "\n",
    "x = keras.layers.Conv1D(32, kernel_size=3, name='Conv_2')(x)\n",
    "x = keras.layers.LeakyReLU(0.1)(x)\n",
    "\n",
    "\n",
    "x = keras.layers.Conv1D(32, kernel_size=3, name='Conv_3')(x)\n",
    "x = keras.layers.LeakyReLU(0.1)(x)\n",
    "\n",
    "# flatten\n",
    "x = keras.layers.Flatten(name='Flatten')(x)\n",
    "\n",
    "# Fully connected hidden layers\n",
    "x = keras.layers.Dense(1024, name='Dense_1')(x)\n",
    "x = keras.layers.ReLU(name='ReLU_dense_1')(x)\n",
    "\n",
    "x = keras.layers.Dense(512, name='Dense_2')(x)\n",
    "x = keras.layers.ReLU(name='ReLU_dense_2')(x)\n",
    "\n",
    "action = keras.layers.Dense(num_actions, activation='softmax', name='Output')(x)\n",
    "critic = keras.layers.Dense(1)(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs= [action, critic ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up of Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set up of AC model\n",
    "# num_inputs = 80      # board state, theres 80 options \n",
    "# num_actions = 142    # taking out some invalid moves\n",
    "# num_hidden_1 = 1024  # first hidden layer\n",
    "# num_hidden_2 = 512   # second hidden layer \n",
    "\n",
    "\n",
    "# inputs = layers.Input(shape=(num_inputs,))\n",
    "\n",
    "# #first hidden layer\n",
    "# common_1 = layers.Dense(num_hidden_1, activation=\"relu\")(inputs)\n",
    "\n",
    "# #second hidden layer\n",
    "# common_2 = layers.Dense(num_hidden_2, activation=\"relu\")(common_1)\n",
    "\n",
    "# # choosing action \n",
    "# action = layers.Dense(num_actions, activation=\"softmax\")(common_2)\n",
    "# # outputting critic score  \n",
    "# critic = layers.Dense(1)(common_2)\n",
    "\n",
    "# model = keras.Model(inputs=inputs, outputs=[action, critic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Random Choices from AC NN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info from end game call back "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------\n",
      "Seed: 305171158494025747926995200363878327406\n",
      "Score: -5 , moves left: 24 \n",
      "Score: -5 , moves left: 23 \n",
      "Score: -5 , moves left: 22 \n",
      "Score: -5 , moves left: 21 \n",
      "Score: -5 , moves left: 20 \n",
      "Score: -5 , moves left: 19 \n",
      "Score: 8 , moves left: 18 \n",
      "Score: -5 , moves left: 17 \n",
      "Score: 8 , moves left: 16 \n",
      "Score: -5 , moves left: 15 \n",
      "Score: -5 , moves left: 14 \n",
      "Score: -5 , moves left: 13 \n",
      "Score: 8 , moves left: 12 \n",
      "Score: -5 , moves left: 11 \n",
      "Score: -5 , moves left: 10 \n",
      "Score: -5 , moves left: 9 \n",
      "Score: -5 , moves left: 8 \n",
      "Score: -5 , moves left: 7 \n",
      "Score: -5 , moves left: 6 \n",
      "Score: -5 , moves left: 5 \n",
      "Score: -5 , moves left: 4 \n",
      "Score: -5 , moves left: 3 \n",
      "Score: -5 , moves left: 2 \n",
      "Score: -5 , moves left: 1 \n",
      "Score: -5 , moves left: 0 \n",
      "\n",
      "LENGTH BOARDS:  26\n",
      "\n",
      "Scores:  [0, -5, -10, -15, -20, -25, -30, -22, -27, -19, -24, -29, -34, -26, -31, -36, -41, -46, -51, -56, -61, -66, -71, -76, -81, -86]\n",
      "\n",
      "Score delta [-5, -5, -5, -5, -5, -5, 8, -5, 8, -5, -5, -5, 8, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5]\n",
      "\n",
      "Moves:  [(1, 6, 0), (3, 5, 1), (4, 4, 0), (5, 1, 0), (5, 5, 0), (3, 3, 0), (5, 9, 0), (6, 2, 1), (6, 7, 0), (6, 1, 0), (1, 8, 0), (4, 7, 1), (4, 8, 0), (6, 2, 1), (2, 7, 0), (7, 5, 1), (7, 2, 1), (2, 6, 1), (6, 3, 1), (6, 7, 1), (2, 1, 1), (0, 1, 1), (4, 3, 1), (0, 3, 1), (5, 5, 0)]\n",
      "\n",
      "FINAL SCORE:  -86\n",
      "--------------------------------------------------------------\n",
      "Seed: 279156767310958833831063453745835197956\n",
      "Score: -5 , moves left: 24 \n",
      "Score: -5 , moves left: 23 \n",
      "Score: -5 , moves left: 22 \n",
      "Score: -5 , moves left: 21 \n",
      "Score: -5 , moves left: 20 \n",
      "Score: -5 , moves left: 19 \n",
      "Score: -5 , moves left: 18 \n",
      "Score: -5 , moves left: 17 \n",
      "Score: -5 , moves left: 16 \n",
      "Score: -5 , moves left: 15 \n",
      "Score: -5 , moves left: 14 \n",
      "Score: -5 , moves left: 13 \n",
      "Score: -5 , moves left: 12 \n",
      "Score: 8 , moves left: 11 \n",
      "Score: -5 , moves left: 10 \n",
      "Score: -5 , moves left: 9 \n",
      "Score: -5 , moves left: 8 \n",
      "Score: -5 , moves left: 7 \n",
      "Score: -5 , moves left: 6 \n",
      "Score: -5 , moves left: 5 \n",
      "Score: -5 , moves left: 4 \n",
      "Score: -5 , moves left: 3 \n",
      "Score: 31 , moves left: 2 \n",
      "Score: -5 , moves left: 1 \n",
      "Score: -5 , moves left: 0 \n",
      "\n",
      "LENGTH BOARDS:  26\n",
      "\n",
      "Scores:  [0, -5, -10, -15, -20, -25, -30, -35, -40, -45, -50, -55, -60, -65, -57, -62, -67, -72, -77, -82, -87, -92, -97, -66, -71, -76]\n",
      "\n",
      "Score delta [-5, -5, -5, -5, -5, -5, 8, -5, 8, -5, -5, -5, 8, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, 8, -5, -5, -5, -5, -5, -5, -5, -5, 31, -5, -5]\n",
      "\n",
      "Moves:  [(2, 2, 1), (6, 8, 0), (0, 1, 0), (1, 7, 1), (3, 4, 1), (3, 4, 0), (3, 0, 0), (6, 9, 1), (1, 9, 0), (3, 1, 0), (3, 5, 1), (1, 2, 0), (0, 9, 0), (0, 7, 1), (3, 7, 1), (1, 2, 1), (4, 9, 1), (0, 2, 0), (2, 7, 0), (4, 1, 1), (6, 9, 1), (6, 6, 0), (3, 6, 0), (6, 6, 0), (5, 9, 1)]\n",
      "\n",
      "FINAL SCORE:  -76\n",
      "--------------------------------------------------------------\n",
      "Seed: 124655698662667018733482243662344521702\n",
      "Score: -5 , moves left: 24 \n",
      "Score: -5 , moves left: 23 \n",
      "Score: -5 , moves left: 22 \n",
      "Score: -5 , moves left: 21 \n",
      "Score: -5 , moves left: 20 \n",
      "Score: -5 , moves left: 19 \n",
      "Score: -5 , moves left: 18 \n",
      "Score: -5 , moves left: 17 \n",
      "Score: -5 , moves left: 16 \n",
      "Score: -5 , moves left: 15 \n",
      "Score: -5 , moves left: 14 \n",
      "Score: -5 , moves left: 13 \n",
      "Score: -5 , moves left: 12 \n",
      "Score: -5 , moves left: 11 \n",
      "Score: -5 , moves left: 10 \n",
      "Score: -5 , moves left: 9 \n",
      "Score: -5 , moves left: 8 \n",
      "Score: -5 , moves left: 7 \n",
      "Score: -5 , moves left: 6 \n",
      "Score: -5 , moves left: 5 \n",
      "Score: -5 , moves left: 4 \n",
      "Score: -5 , moves left: 3 \n",
      "Score: 8 , moves left: 2 \n",
      "Score: -5 , moves left: 1 \n",
      "Score: -5 , moves left: 0 \n",
      "\n",
      "LENGTH BOARDS:  26\n",
      "\n",
      "Scores:  [0, -5, -10, -15, -20, -25, -30, -35, -40, -45, -50, -55, -60, -65, -70, -75, -80, -85, -90, -95, -100, -105, -110, -102, -107, -112]\n",
      "\n",
      "Score delta [-5, -5, -5, -5, -5, -5, 8, -5, 8, -5, -5, -5, 8, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, 8, -5, -5, -5, -5, -5, -5, -5, -5, 31, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, 8, -5, -5]\n",
      "\n",
      "Moves:  [(1, 6, 0), (3, 4, 1), (4, 3, 1), (1, 6, 0), (4, 3, 1), (7, 7, 1), (1, 5, 1), (0, 6, 1), (0, 0, 0), (3, 4, 1), (3, 2, 1), (5, 5, 0), (4, 8, 0), (6, 4, 1), (5, 0, 0), (5, 6, 1), (2, 5, 0), (1, 4, 0), (5, 1, 1), (4, 2, 0), (3, 8, 0), (5, 1, 0), (4, 3, 0), (6, 1, 0), (7, 7, 1)]\n",
      "\n",
      "FINAL SCORE:  -112\n",
      "--------------------------------------------------------------\n",
      "Seed: 220583960317392462691858539188089884366\n",
      "Score: -5 , moves left: 24 \n",
      "Score: -5 , moves left: 23 \n",
      "Score: -5 , moves left: 22 \n",
      "Score: -5 , moves left: 21 \n",
      "Score: -5 , moves left: 20 \n",
      "Score: -5 , moves left: 19 \n",
      "Score: -5 , moves left: 18 \n",
      "Score: -5 , moves left: 17 \n",
      "Score: -5 , moves left: 16 \n",
      "Score: -5 , moves left: 15 \n",
      "Score: -5 , moves left: 14 \n",
      "Score: -5 , moves left: 13 \n",
      "Score: -5 , moves left: 12 \n",
      "Score: -5 , moves left: 11 \n",
      "Score: -5 , moves left: 10 \n",
      "Score: -5 , moves left: 9 \n",
      "Score: -5 , moves left: 8 \n",
      "Score: -5 , moves left: 7 \n",
      "Score: -5 , moves left: 6 \n",
      "Score: -5 , moves left: 5 \n",
      "Score: -5 , moves left: 4 \n",
      "Score: -5 , moves left: 3 \n",
      "Score: -5 , moves left: 2 \n",
      "Score: -5 , moves left: 1 \n",
      "Score: -5 , moves left: 0 \n",
      "\n",
      "LENGTH BOARDS:  26\n",
      "\n",
      "Scores:  [0, -5, -10, -15, -20, -25, -30, -35, -40, -45, -50, -55, -60, -65, -70, -75, -80, -85, -90, -95, -100, -105, -110, -115, -120, -125]\n",
      "\n",
      "Score delta [-5, -5, -5, -5, -5, -5, 8, -5, 8, -5, -5, -5, 8, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, 8, -5, -5, -5, -5, -5, -5, -5, -5, 31, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, 8, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5]\n",
      "\n",
      "Moves:  [(1, 4, 0), (1, 8, 1), (0, 1, 1), (3, 4, 1), (1, 7, 1), (3, 8, 0), (3, 5, 1), (0, 7, 0), (3, 8, 0), (6, 8, 0), (6, 9, 1), (3, 8, 0), (6, 7, 1), (0, 3, 1), (4, 2, 1), (3, 8, 1), (5, 4, 0), (3, 7, 0), (2, 8, 0), (3, 8, 1), (6, 4, 1), (6, 2, 0), (6, 1, 1), (5, 1, 1), (0, 5, 0)]\n",
      "\n",
      "FINAL SCORE:  -125\n",
      "--------------------------------------------------------------\n",
      "Seed: 108336822558459450296109477311004508706\n",
      "Score: -5 , moves left: 24 \n",
      "Score: 16 , moves left: 23 \n",
      "Score: -5 , moves left: 22 \n",
      "Score: -5 , moves left: 21 \n",
      "Score: -5 , moves left: 20 \n",
      "Score: -5 , moves left: 19 \n",
      "Score: -5 , moves left: 18 \n",
      "Score: -5 , moves left: 17 \n",
      "Score: -5 , moves left: 16 \n",
      "Score: 31 , moves left: 15 \n",
      "Score: -5 , moves left: 14 \n",
      "Score: -5 , moves left: 13 \n",
      "Score: -5 , moves left: 12 \n",
      "Score: -5 , moves left: 11 \n",
      "Score: -5 , moves left: 10 \n",
      "Score: -5 , moves left: 9 \n",
      "Score: -5 , moves left: 8 \n",
      "Score: -5 , moves left: 7 \n",
      "Score: -5 , moves left: 6 \n",
      "Score: -5 , moves left: 5 \n",
      "Score: 16 , moves left: 4 \n",
      "Score: -5 , moves left: 3 \n",
      "Score: -5 , moves left: 2 \n",
      "Score: 8 , moves left: 1 \n",
      "Score: -5 , moves left: 0 \n",
      "\n",
      "LENGTH BOARDS:  26\n",
      "\n",
      "Scores:  [0, -5, 11, 6, 1, -4, -9, -14, -19, -24, 7, 2, -3, -8, -13, -18, -23, -28, -33, -38, -43, -27, -32, -37, -29, -34]\n",
      "\n",
      "Score delta [-5, -5, -5, -5, -5, -5, 8, -5, 8, -5, -5, -5, 8, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, 8, -5, -5, -5, -5, -5, -5, -5, -5, 31, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, 8, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, 16, -5, -5, -5, -5, -5, -5, -5, 31, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, 16, -5, -5, 8, -5]\n",
      "\n",
      "Moves:  [(0, 3, 1), (3, 7, 1), (3, 7, 0), (1, 9, 0), (2, 9, 0), (7, 1, 1), (0, 6, 0), (1, 2, 0), (0, 4, 1), (5, 3, 1), (1, 7, 0), (2, 8, 0), (6, 7, 0), (0, 2, 1), (6, 2, 1), (1, 0, 0), (4, 3, 1), (3, 9, 1), (0, 0, 0), (2, 9, 1), (0, 7, 0), (3, 6, 1), (1, 9, 0), (3, 5, 0), (1, 5, 1)]\n",
      "\n",
      "FINAL SCORE:  -34\n"
     ]
    }
   ],
   "source": [
    "# Info run of AC NN \n",
    "\n",
    "# perfectly fine random \n",
    "\n",
    "import random\n",
    "import graphical, game\n",
    "\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "scores_delta = []\n",
    "scores_end = []\n",
    "counter = 0\n",
    "runs = 2\n",
    "\n",
    "def ai_callback(board, score, moves_left):\n",
    "    \n",
    "    # pre-process board \n",
    "    integer_board = preprocess_string_int(board)\n",
    "        \n",
    "    # now actor critic to make decision of choice \n",
    "    action_probs, critic_val = model(integer_board)\n",
    "    \n",
    "    # storing critic val results\n",
    "    critic_value_history.append(critic_val[0, 0])\n",
    "    \n",
    "    # Sample action from action probability distribution, higher probababilites more chance of getting selected\n",
    "    action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "    \n",
    "    # storing action probs results\n",
    "    action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "    \n",
    "    return dictOfActions[action]\n",
    "            \n",
    "    \n",
    "def transition_callback(board, move, score_delta, next_board, moves_left):\n",
    "    scores_delta.append(score_delta)\n",
    "    print(f\"Score: {score_delta} , moves left: {moves_left} \")\n",
    "    #pass # This can be used to monitor outcomes of moves\n",
    "\n",
    "    \n",
    "    \n",
    "def end_of_game_callback(boards, scores, moves, final_score):\n",
    "    \n",
    "    scores_end.append(scores)\n",
    "\n",
    "    boards_ints = []\n",
    "\n",
    "    for i in boards:\n",
    "        boards_ints.append(preprocess_string_int(i))\n",
    "\n",
    "    print(\"\\nLENGTH BOARDS: \" , len(boards))\n",
    "\n",
    "    print(\"\\nScores: \" , scores)\n",
    "\n",
    "    print(\"\\nScore delta\" , scores_delta)\n",
    "\n",
    "    print(\"\\nMoves: \" ,  moves)\n",
    "\n",
    "    print(\"\\nFINAL SCORE: \" , final_score)\n",
    "\n",
    "    return False # True = play another, False = Done\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "          \n",
    "    # repeat for x times , range(x) \n",
    "    \n",
    "    for i in range(5):\n",
    "        print(\"---------------------------------------------------------------------------------------------------\")\n",
    "        speedup = 10\n",
    "        g = graphical.Game(ai_callback, transition_callback, end_of_game_callback, speedup)\n",
    "        g.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What\n",
    "\n",
    "So we know the board throughout the 25 moves , running score of each move & individual score, and finally the move itself  \n",
    "\n",
    "Also can keep a list of action probs and critic vales from ai_call back \n",
    "\n",
    "Need to train/update weights on this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to update weights using keras example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 269661543592309657617714846807087655832\n",
      "Score: -5 , moves left: 24 \n",
      "Score: -5 , moves left: 23 \n",
      "Score: -5 , moves left: 22 \n",
      "Score: -5 , moves left: 21 \n",
      "Score: -5 , moves left: 20 \n",
      "Score: -5 , moves left: 19 \n",
      "Score: -5 , moves left: 18 \n",
      "Score: -5 , moves left: 17 \n",
      "Score: -5 , moves left: 16 \n",
      "Score: -5 , moves left: 15 \n",
      "Score: 16 , moves left: 14 \n",
      "Score: -5 , moves left: 13 \n",
      "Score: -5 , moves left: 12 \n",
      "Score: -5 , moves left: 11 \n",
      "Score: -5 , moves left: 10 \n",
      "Score: -5 , moves left: 9 \n",
      "Score: -5 , moves left: 8 \n",
      "Score: -5 , moves left: 7 \n",
      "Score: -5 , moves left: 6 \n",
      "Score: -5 , moves left: 5 \n",
      "Score: 8 , moves left: 4 \n",
      "Score: 54 , moves left: 3 \n",
      "Score: -5 , moves left: 2 \n",
      "Score: -5 , moves left: 1 \n",
      "Score: -5 , moves left: 0 \n",
      "Loss val: tf.Tensor(-27.530937, shape=(), dtype=float32)\n",
      "Grads :  [None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['Conv_1/kernel:0', 'Conv_1/bias:0', 'Conv_2/kernel:0', 'Conv_2/bias:0', 'Conv_3/kernel:0', 'Conv_3/bias:0', 'Dense_1/kernel:0', 'Dense_1/bias:0', 'Dense_2/kernel:0', 'Dense_2/bias:0', 'Output/kernel:0', 'Output/bias:0', 'dense/kernel:0', 'dense/bias:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-12c156758f39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[0mspeedup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraphical\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mai_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransition_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_of_game_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspeedup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m     \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\MSc AI & ML\\Sem2\\RL\\Candy Crush\\graphical.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    257\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_in_state\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mWAIT_AFTER_MOVE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_gameover\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay_another\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend_of_game_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_enter_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mST_POSTGAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-12c156758f39>\u001b[0m in \u001b[0;36mend_of_game_callback\u001b[1;34m(boards, scores, moves, final_score)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Grads : \"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;31m# Clear the loss and reward history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\candy\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[0mRuntimeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mcalled\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcross\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mreplica\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m     \"\"\"\n\u001b[1;32m--> 598\u001b[1;33m     \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_empty_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\candy\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\utils.py\u001b[0m in \u001b[0;36mfilter_empty_gradients\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m     77\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[1;32m---> 79\u001b[1;33m                      ([v.name for _, v in grads_and_vars],))\n\u001b[0m\u001b[0;32m     80\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     logging.warning(\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: ['Conv_1/kernel:0', 'Conv_1/bias:0', 'Conv_2/kernel:0', 'Conv_2/bias:0', 'Conv_3/kernel:0', 'Conv_3/bias:0', 'Dense_1/kernel:0', 'Dense_1/bias:0', 'Dense_2/kernel:0', 'Dense_2/bias:0', 'Output/kernel:0', 'Output/bias:0', 'dense/kernel:0', 'dense/bias:0']."
     ]
    }
   ],
   "source": [
    "import random\n",
    "import graphical, game\n",
    "\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "scores_delta = []\n",
    "\n",
    "# Discount factor for past rewards\n",
    "gamma = 0.99  \n",
    "\n",
    "# Smallest number such that 1.0 + eps != 1.0\n",
    "eps = np.finfo(np.float32).eps.item()  \n",
    "\n",
    "\n",
    "\n",
    "def ai_callback(board, score, moves_left):\n",
    "    \n",
    "    # pre-process board \n",
    "    integer_board = preprocess_string_int(board)\n",
    "\n",
    "    # now actor critic to make decision of choice \n",
    "    action_probs, critic_val = model(integer_board)\n",
    "    \n",
    "    # storing results\n",
    "    critic_value_history.append(critic_val[0, 0])\n",
    "\n",
    "    # Sample action from action probability distribution, higher probababilites more chance of getting selected\n",
    "    action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "    # storing results\n",
    "    action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "    \n",
    "         \n",
    "    return dictOfActions[action]\n",
    "            \n",
    "    \n",
    "def transition_callback(board, move, score_delta, next_board, moves_left):\n",
    "    scores_delta.append(score_delta)\n",
    "    print(f\"Score: {score_delta} , moves left: {moves_left} \")\n",
    "    #pass # This can be used to monitor outcomes of moves\n",
    "\n",
    "    \n",
    "def end_of_game_callback(boards, scores, moves, final_score):\n",
    "       \n",
    "    running_reward = 0\n",
    "    episode_reward = 0\n",
    "    reward_history = [] \n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "    huber_loss = keras.losses.Huber()\n",
    "    \n",
    "\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "\n",
    "        rewards_history = scores\n",
    "        episode_reward = final_score\n",
    "\n",
    "        # Calculate expected value from rewards\n",
    "        # - At each timestep what was the total reward received after that timestep\n",
    "        # - Rewards in the past are discounted by multiplying them with gamma\n",
    "        # - These are the labels for our critic\n",
    "        \n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            \n",
    "            #print(\"Discount sum: \" , discounted_sum)\n",
    "            \n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        # Normalize\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "        \n",
    "        #print(\"RET: \" , returns)\n",
    "\n",
    "\n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "\n",
    "\n",
    "        for log_prob, value, ret in history:\n",
    "            # At this point in history, the critic estimated that we would get a\n",
    "            # total reward = `value` in the future. We took an action with log probability\n",
    "            # of `log_prob` and ended up recieving a total reward = `ret`.\n",
    "            # The actor must be updated so that it predicts an action that leads to\n",
    "            # high rewards (compared to critic's estimate) with high probability.\n",
    "            diff = ret - value\n",
    "            \n",
    "            #print(\"Diff: \" , diff)\n",
    "            \n",
    "            actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "            # The critic must be updated so that it predicts a better estimate of\n",
    "            # the future rewards.\n",
    "            critic_losses.append(\n",
    "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "            )\n",
    "                \n",
    "        #print(\"crit loss: \" ,critic_losses)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        \n",
    "        #print(\"Loss val:\" , loss_value)\n",
    "        \n",
    "        #print(\"mod train vars: \" ,model.trainable_variables)\n",
    "        \n",
    "        \n",
    "        # these are none ! \n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        \n",
    "        # this is issue always none\n",
    "        print(\"Grads : \" , grads)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Clear the loss and reward history\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "\n",
    "\n",
    "    print(\"Maybe did something\")\n",
    "    return False # True = play another, False = Done\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    speedup = 10\n",
    "    g = graphical.Game(ai_callback, transition_callback, end_of_game_callback, speedup)\n",
    "    g.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
